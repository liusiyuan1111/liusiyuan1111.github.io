<!DOCTYPE html>
<html lang='zh-CN'>

<head>
  <meta name="generator" content="Hexo 5.3.0">
  <meta charset="utf-8">
  

  <meta http-equiv='x-dns-prefetch-control' content='on' />
  <link rel='dns-prefetch' href='https://cdn.jsdelivr.net'>
  <link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>

  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" content="#f8f8f8">
  <title>论文翻译：Deep Image Retrieval  A Survey - 刘思远的博客</title>

  
    <meta name="description" content="Deep Image Retrieval: A Survey深度图像检索综述 Abstract—In recent years a vast amount of visual content has been generated and shared from various fields, such as social media platforms, medical images, and r">
<meta property="og:type" content="article">
<meta property="og:title" content="论文翻译：Deep Image Retrieval  A Survey">
<meta property="og:url" content="http://example.com/2021/02/05/07-%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%EF%BC%9ADeep-Image-Retrieval-A-Survey/index.html">
<meta property="og:site_name" content="刘思远的博客">
<meta property="og:description" content="Deep Image Retrieval: A Survey深度图像检索综述 Abstract—In recent years a vast amount of visual content has been generated and shared from various fields, such as social media platforms, medical images, and r">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/02/05/bM8DLriRHAEPGvU.png">
<meta property="og:image" content="https://i.loli.net/2021/02/05/U36pBOD87Ckujrd.png">
<meta property="article:published_time" content="2021-02-05T11:53:38.000Z">
<meta property="article:modified_time" content="2021-05-23T11:17:16.610Z">
<meta property="article:author" content="Liu">
<meta property="article:tag" content="论文翻译">
<meta property="article:tag" content="图像检索">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="Paper">
<meta property="article:tag" content="CV">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/05/bM8DLriRHAEPGvU.png">
  
  

  <!-- feed -->
  

  
    
<link rel="stylesheet" href="/css/main.css">

  

  

  
</head>

<body>
  


  <div class='l_body' id='start'>
    <aside class='l_left' layout='post'>
    


<header class="header">

<div class="logo-wrap"><a class="title" href="/"><div class="main">刘思远的博客</div><div class="sub cap">没有一个冬天不可逾越，没有一个春天不会来临</div></a></div>
<nav class="menu dis-select"></nav></header>

<div class="widgets">

<div class="widget-wrap single" id="toc"><div class="widget-header cap dis-select"><span class="name">本文目录</span></div><div class="widget-body fs14"><div class="doc-tree active"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-INTRODUCTION"><span class="toc-text">1 INTRODUCTION</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%BC%95%E8%A8%80"><span class="toc-text">1 引言</span></a></li></ol></div></div></div>


</div>


    </aside>
    <div class='l_main'>
      

      


<div class="bread-nav fs12"><div id="breadcrumb"><a class="cap breadcrumb" href="/">刘思远的博客</a><span class="sep"></span><a class="cap breadcrumb" href="/">文章</a></div><div id="post-meta">发布于&nbsp;<time datetime="2021-02-05T11:53:38.000Z">2021-02-05</time></div></div>

<article class='content md post'>
<h1 class="article-title"><span>论文翻译：Deep Image Retrieval  A Survey</span></h1>
<h1 id="Deep-Image-Retrieval-A-Survey"><a href="#Deep-Image-Retrieval-A-Survey" class="headerlink" title="Deep Image Retrieval: A Survey"></a>Deep Image Retrieval: A Survey</h1><p>深度图像检索综述</p>
<p><strong>Abstract</strong>—In recent years a vast amount of visual content has been generated and shared from various fields, such as social media platforms, medical images, and robotics. This abundance of content creation and sharing has introduced new challenges. In particular, searching databases for similar content, i.e., content based image retrieval (CBIR), is a long-established research area, and more efficient and accurate methods are needed for real time retrieval. Artificial intelligence has made progress in CBIR and has significantly facilitated the process of intelligent search. In this survey we organize and review recent CBIR works that are developed based on deep learning algorithms and techniques, including insights and techniques from recent papers. We identify and present the commonly-used benchmarks and evaluation methods used in the field. We collect common challenges and propose promising future directions. More specifically, we focus on image retrieval with deep learning and organize the state of the art methods according to the types of deep network structure, deep features, feature enhancement methods, and network fine-tuning strategies. Our survey considers a wide variety of recent methods, aiming to promote a global view of the field of instance-based CBIR.<br><strong>Index Terms</strong>—Content based image retrieval, Deep learning, Convolutional neural networks, Literature survey</p>
<p><strong>摘要</strong>—近年来，从社交媒体平台、医学图像和机器人技术等各个领域产生并共享了大量视觉内容。这种丰富的内容创建和共享带来了新的挑战。特别是，在数据库中搜索相似的内容，即基于内容的图像检索(CBIR)，是一个由来已久的研究领域，实时检索需要更有效和更准确的方法。人工智能在CBIR取得了进展，极大地促进了智能搜索的进程。在这篇综述中，我们组织和回顾了CBIR最近的工作，这些工作是基于深度学习算法和技术开发的，包括来自最近论文的算法和技术。我们确定并介绍了该领域常用的基准和评估方法。我们收集共同的挑战，并提出有前景的未来研究方向。更具体地说，我们关注具有深度学习的图像检索，并根据深度网络结构、深度特征、特征增强方法和网络微调策略的类型来组织最先进的方法。我们的综述考虑了各种各样的最新方法，旨在促进基于实例的CBIR领域的全球视野。</p>
<p><strong>索引</strong>—基于内容的图像检索、深度学习、卷积神经网络、文献综述</p>
<h2 id="1-INTRODUCTION"><a href="#1-INTRODUCTION" class="headerlink" title="1 INTRODUCTION"></a><strong>1 INTRODUCTION</strong></h2><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a><strong>1 引言</strong></h2><p>CONTENT based image retrieval (CBIR) is the problem of searching for semantically matched or similar images in a large image gallery by analyzing their visual content, given a query image that describes the user’s needs. CBIR has been a longstanding research topic in the computer vision and multi-media community [1], [2]. With the present, exponentially increasing, amount of image and video data, the development of appropriate information systems that efficiently manage such large image collections is of utmost importance, with image searching being one of the most indispensable techniques. Thus there is nearly endless potential for applications of CBIR, such as person re-identification [3], remote sensing [4], medical image search [5], and shopping recommendation in online markets [6], among many others.</p>
<p>基于内容的图像检索(CBIR)是通过分析图像的视觉内容，在大型图像库中搜索语义匹配或相似的图像的问题，给出描述用户需求的查询图像。CBIR一直是计算机视觉和多媒体领域的一个长期研究课题。随着当前图像和视频数据量的指数级增长，开发能够有效管理如此大的图像集合的信息系统是至关重要的，其中图像检索是不可缺少的技术之一。因此，CBIR的应用潜力几乎是无穷无尽的，如行人重识别，遥感图像，医学图像检索，以及在线商城的购物推荐等等。</p>
<p>A broad categorization of CBIR methodologies depends on the level of retrieval, i.e., instance level and category level. In instance level image retrieval, a query image of a particular object or scene (e.g., the Eiffel Tower) is given and the goal is to find images containing the same object or scene that may be captured under different conditions [7], [8]. In contrast, the goal of category level retrieval is to find images of the same class as the query (e.g., dogs, cars, etc.). Instance level retrieval is more challenging and promising as it satisfies specific objectives for many applications. Notice that we limit the focus of this survey to instance-level image retrieval and in the following, if not further specified, “image retrieval” and “instance retrieval” are considered equivalent and will be used interchangeably. </p>
<p>CBIR方法的广泛分类取决于检索的级别，即实例级和类别级。在实例级图像检索中，给出特定对象或场景(例如，埃菲尔铁塔)的查询图像，并且目标是找到包含在不同条件下可能被捕获的相同对象或场景的图像。相比之下，类别级检索的目标是找到与查询相同类别的图像(例如，狗、汽车等))。实例级检索更具挑战性和前景，因为它满足了许多应用程序的特定目标。注意我们将本次综述的重点限于实例级图像检索，在下文中，如果没有进一步说明，“图像检索”和“实例检索”被认为是等效的，将可互换使用。</p>
<p>Finding a desired image can require a search among thousands, millions, or even billions of images. Hence, searching efficiently is as critical as searching accurately, to which continued efforts have been devoted [7], [8], [9], [10], [11].To enable accurate and efficient retrieval of massive image collections,compact yet rich feature representations are at the core of CBIR.</p>
<p>找到想要的图像可能需要在数千、数百万甚至数十亿张图像中进行搜索。因此，有效的搜索和精确的搜索一样重要，对此人们一直在努力，参考文献7-11。为了准确高效地检索大量图像集合，紧凑而丰富的特征表示是CBIR的核心。</p>
<p>In the past two decades, remarkable progress has been made in image feature representations, which mainly consist of two important periods: feature engineering and feature learning (particularly deep learning). In the feature engineering era (i.e., pre-deep learning), the field was dominated by milestone hand-engineered feature descriptors, such as the Scale-Invariant Feature Transform (SIFT) [19]. The feature learning stage, the deep learning era since 2012, begins with artificial neural networks, particularly the breakthrough ImageNet and the Deep Convolutional Neural Network (DCNN) AlexNet [20]. Since then, deep learning has impacted a broad range of research areas, since DCNNs can learn powerful feature representations with multiple levels of abstraction directly from data. Deep learning techniques have attracted enormous attention and have brought about considerable breakthroughs in many computer vision tasks, including image classification [20], [21], [22], object detection [23], and image retrieval [10], [13], [14].</p>
<p>在过去的二十年里，图像特征表示取得了显著的进展，主要包括两个重要阶段:特征工程和特征学习(特别是深度学习)。在特征工程时代(即深度学习前)，该领域由里程碑式的手工工程特征描述符主导，如尺度不变特征变换(SIFT) 。特征学习阶段，自2012年以来的深度学习时代，始于人工神经网络，特别是突破性的ImageNet和深度卷积神经网络(DCNN) AlexNet  。从那以后，深度学习影响了广泛的研究领域，因为数据挖掘神经网络可以直接从数据中学习具有多层次抽象的强大特征表示。深度学习技术已经引起了极大的关注，并在许多计算机视觉任务中带来了相当大的突破，包括图像分类、对象检测和图像检索。</p>
<p>Excellent surveys for traditional image retrieval can be found in [1], [2], [8]. This paper, in contrast, focuses on deep learning based methods. A comparison of our work with other published surveys [8], [14], [15], [16] is shown in Table 1. Deep learning for image retrieval is comprised of the essential stages shown in Figure 1 and various methods, focusing on one or more stages, have been proposed to improve retrieval accuracy and efficiency. In this survey, we include comprehensive details about these methods, including feature fusion methods and network fine-tuning strategies etc. , motivated by the following questions that have been driving research in this domain:</p>
<p>传统图像检索的方法可以在综述[1]、[2]、[8]中找到。相比之下，本文侧重于基于深度学习的方法。我们的工作与其他已发表的综述[8]、[14]、[15]、[16]的比较见表1。图像检索的深度学习由图1所示的基本阶段组成，并且已经提出了各种方法，集中在一个或多个阶段，以提高检索的准确性和效率。在本篇综述中，包括了这些方法的综合细节，特征融合方法和网络微调策略等，其动机是推动该领域研究的以下问题:</p>
<ol>
<li>By using off-the-shelf models only, how do deep features outperform hand-crafted features?</li>
<li>In case of domain shifts across training datasets, how can we adapt off-the-shelf models to maintain or even improve<br>retrieval performance?</li>
<li>Since deep features are generally high-dimensional, how can we effectively utilize them to perform efficient image<br>retrieval, especially for large-scale datasets?</li>
</ol>
<p>1)仅使用现成的模型，深度特征如何胜过手工制作的特征？</p>
<p>2)在跨训练数据集的领域转移的情况下，我们如何适应现成的模型来保持甚至提高检索性能？</p>
<p>3)由于深度特征通常是高维的，我们如何有效地利用它们来执行高效的图像检索，尤其是对于大规模数据集。</p>
<p>TABLE 1: A summary and comparison of the primary surveys in the field of image retrieval.</p>
<p>表1:图像检索领域主要综述文献的总结和比较。</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://i.loli.net/2021/02/05/bM8DLriRHAEPGvU.png" alt="image-20210205203313875"></p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://i.loli.net/2021/02/05/U36pBOD87Ckujrd.png" alt="image-20210205203545671"></p>
<p>Fig. 1: In deep image retrieval, feature embedding and aggregation methods are used to enhance the discrimination of deep features. Similarity is measured on these enhanced features using Euclidean or Hamming distances.</p>
<p>图1:在深度图像检索中，使用特征嵌入和聚集方法来增强深度特征的区分度。使用欧几里德距离或汉明距离来测量这些增强特征的相似性。</p>


<div class="article-footer reveal fs14"><section id="license"><div class="header"><span>许可协议</span></div><div class="body"><p>本文采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">署名-非商业性使用-相同方式共享 4.0 国际</a> 许可协议，转载请注明出处。</p>
</div></section></div>

</article>

<div class="related-wrap reveal" id="read-next"><section class="header cap theme"><span>接下来阅读</span></section><section class="body fs14"><a id="next" href="/2021/01/17/06-win10-Linux-manjaro-%E5%8F%8C%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85/">win10+Linux(manjaro)双系统安装<span class="note">较早</span></a><div class="line"></div><a id="prev" href="/2021/05/14/08-%E5%AA%92%E4%BD%93%E6%9F%A5%E8%AF%A2+rem%E5%B8%83%E5%B1%80/">媒体查询+rem布局<span class="note">较新</span></a><div class="line"></div><a id="more" href="/archives">检索全部文章</a></section></div>








      
<footer class="page-footer reveal fs12"><hr><div class="text"><p>本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议，转载请注明出处。</p>
<p>本站由 <a href="http://example.com/">@Liu</a> 创建，使用 <a target="_blank" rel="noopener" href="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.5.1" title="v1.5.1">Stellar</a> 作为主题。</p>
</div></footer>

      <div class='float-panel mobile-only blur' style='display:none'>
  <button type='button' class='sidebar-toggle mobile' onclick='sidebar.toggle()'>
    <svg class="icon" style="width: 1em; height: 1em;vertical-align: middle;fill: currentColor;overflow: hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="15301"><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 2.3 26.8 24.6 47.5 51.6 47.6h416.5v4z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15302"></path><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 1.9 27.7 23.9 49.7 51.6 51.6h416.5z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15303"></path></svg>
  </button>
</div>

    </div>
  </div>
  <div class='scripts'>
    <script type="text/javascript">
  stellar = {
    // 懒加载 css https://github.com/filamentgroup/loadCSS
    loadCSS: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    // 从 butterfly 和 volantis 获得灵感
    loadScript: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // 默认异步，如果需要同步，第二个参数传入 {} 即可
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    // https://github.com/jerryc127/hexo-theme-butterfly
    jQuery: (fn) => {
      if (typeof jQuery === 'undefined') {
        stellar.loadScript(stellar.plugins.jQuery).then(fn)
      } else {
        fn()
      }
    }
  };
  stellar.github = 'https://github.com/xaoxuu/hexo-theme-stellar/tree/1.5.1';
  stellar.config = {
    date_suffix: {
      just: '刚刚',
      min: '分钟前',
      hour: '小时前',
      day: '天前',
      month: '个月前',
    },
  };

  // required plugins (only load if needs)
  stellar.plugins = {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js',
    sitesjs: '/js/plugins/sites.js',
    friendsjs: '/js/plugins/friends.js',
  };

  // optional plugins
  if ('true' == 'true') {
    stellar.plugins.lazyload = Object.assign({"enable":true,"js":"https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.3.1/dist/lazyload.min.js","transition":"blur"});
  }
  if ('true' == 'true') {
    stellar.plugins.swiper = Object.assign({"enable":true,"css":"https://unpkg.com/swiper/swiper-bundle.min.css","js":"https://unpkg.com/swiper/swiper-bundle.min.js"});
  }
  if ('' == 'true') {
    stellar.plugins.scrollreveal = Object.assign({"enable":null,"js":"https://cdn.jsdelivr.net/npm/scrollreveal@4.0.9/dist/scrollreveal.min.js","distance":"8px","duration":500,"interval":100,"scale":1});
  }
  if ('true' == 'true') {
    stellar.plugins.preload = Object.assign({"enable":true,"service":"flying_pages","instant_page":"https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@4.1.2/js/instant_page.js","flying_pages":"https://cdn.jsdelivr.net/gh/gijo-varghese/flying-pages@2.1.2/flying-pages.min.js"});
  }
</script>

<!-- required -->

  
<script src="/js/main.js" async></script>



<!-- optional -->



<!-- inject -->


  </div>
</body>
</html>
